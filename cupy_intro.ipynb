{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cupy](resources/cupy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is CuPy?\n",
    "\n",
    "A NumPy-compatible array library accelerated by CUDA\n",
    "\n",
    "### Top features:\n",
    "- open source [https://github.com/cupy/cupy](https://github.com/cupy/cupy)\n",
    "- high performance with cuda\n",
    "- highly compatible with numpy\n",
    "- easy to install\n",
    "- easy to write a custom kernel\n",
    "- docs [https://docs.cupy.dev/en/stable/](https://docs.cupy.dev/en/stable/)\n",
    "- MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, description=None):\n",
    "        self.description = description\n",
    "        self.steps = {}\n",
    "    def __call__( self, step_name=None):\n",
    "        if step_name not in self.steps.keys():\n",
    "            self.steps[step_name] = 0\n",
    "        return self\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        val = self.end - self.start\n",
    "        key = self._last_key()\n",
    "        if key:\n",
    "            self.steps[key] += val\n",
    "        print(f\"{val}\")\n",
    "    def _last_key(self):\n",
    "        return list(self.steps.keys())[-1] if len(self.steps.keys()) > 0 else None\n",
    "    @property\n",
    "    def total(self):\n",
    "        return sum(list(self.steps.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API:\n",
    "- compatible GPU alternative of `numpy.ndarray` \\\n",
    "  `x_gpu = cp.array([1, 2, 3])` vs. `x_cpu = np.array([1, 2, 3])`\n",
    "- most of the array manipulations are also done \\\n",
    "  `cp.linalg.norm(x_gpu)` vs. `np.linalg.norm(x_cpu)`\n",
    "- Reference: [https://docs.cupy.dev/en/stable/reference/index.html#cupy-reference](https://docs.cupy.dev/en/stable/reference/index.html#cupy-reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu0 = cp.array([1, 2, 3, 4, 5])\n",
    "arr = cp.vstack([x_gpu0]*5)\n",
    "print('array:\\n', arr)\n",
    "print('det:', cp.linalg.det(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CuPy has a concept of the *current device*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu0 = cp.array([1, 2, 3, 4, 5])\n",
    "print(f'x_gpu0 device: {x_gpu0.device}\\n')\n",
    "\n",
    "cp.cuda.Device(1).use()\n",
    "x_gpu1 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "with cp.cuda.Device(1):\n",
    "    x_gpu1 = cp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "with cp.cuda.Device(0):\n",
    "    x_gpu0 * 2\n",
    "\n",
    "with cp.cuda.Device(1):\n",
    "    try:\n",
    "        x_gpu0 * 2 # error\n",
    "    except:\n",
    "        print(f'{sys.exc_info()[0].__name__}: {sys.exc_info()[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy mem: *host* --> *device*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.array([1, 2, 3])\n",
    "x_gpu = cp.asarray(x_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy mem: *device* --> *host*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = cp.asnumpy(x_gpu)\n",
    "# or\n",
    "tmp2 = x_gpu.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "CuPy uses memory pool for memory allocations:\n",
    "- Device memory pool (GPU device memory), which is used for GPU memory allocations.\n",
    "- Pinned memory pool (non-swappable CPU memory), which is used during CPU-to-GPU data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device 1\n",
    "mempool = cp.get_default_memory_pool()\n",
    "pinned_mempool = cp.get_default_pinned_memory_pool()\n",
    "print(f'Used memory on device 1 [bytes]: {mempool.used_bytes()}')\n",
    "# print(f'Pinned memory [bytes]: {pinned_mempool.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    "- Elementwise\n",
    "- Reduction\n",
    "- Raw <--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer('add_vec')\n",
    "vec_size = int(5e7)\n",
    "\n",
    "# Create random GPU vectors\n",
    "with timer(\"GPU_create_random_vectors\"):\n",
    "    vec_a_gpu = cp.random.random(vec_size, dtype=cp.float64)\n",
    "    vec_b_gpu = cp.random.random(vec_size, dtype=cp.float64)\n",
    "    vec_c_gpu = cp.empty(vec_size, dtype=np.float64)\n",
    "with timer(\"CPU_create_random_vectors\"):\n",
    "    vec_a_cpu = np.random.random(vec_size)\n",
    "    vec_b_cpu = np.random.random(vec_size)\n",
    "print(f'Used mem: {vec_a_gpu.nbytes/1024**3*3} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_vec_elem = cp.ElementwiseKernel(\n",
    "    'float64 x, float64 y',\n",
    "    'float64 z',\n",
    "    'z = x + y',\n",
    "    'add_vec_elem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"GPU_add_vectors\"):\n",
    "    vec_c_gpu = add_vec_elem(vec_a_gpu, vec_b_gpu)\n",
    "with timer(\"CPU_add_vectors\"):\n",
    "    vec_c_cpu = np.add(vec_a_cpu, vec_b_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "del vec_a_cpu\n",
    "del vec_b_cpu\n",
    "del vec_c_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU: When the array goes out of scope, the allocated device memory is released and kept in the pool for future reuse.\n",
    "del vec_a_gpu\n",
    "del vec_b_gpu\n",
    "del vec_c_gpu\n",
    "print(f'Used [bytes]: {mempool.used_bytes()}')\n",
    "print(f'Allocated [bytes]: {mempool.total_bytes()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the memory pool\n",
    "mempool.free_all_blocks()\n",
    "\n",
    "print(f'Allocated [bytes]: {mempool.total_bytes()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Something big :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat random 3D array in GPU RAM (select GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tuple([1024]*3)\n",
    "cp.cuda.Device(0).use()\n",
    "mempool = cp.get_default_memory_pool()\n",
    "timer = Timer('smth_big')\n",
    "\n",
    "with timer(\"GPU_create_random_array\"):\n",
    "    x_gpu = cp.random.random(batch)\n",
    "print(f'Used [GB]: {mempool.used_bytes()/1024**3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"CPU_create_random_array\"):\n",
    "    x_cpu = np.random.random(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"GPU_do_stuff\"):\n",
    "    x_gpu *= 3\n",
    "    x_gpu *= x_gpu\n",
    "with timer(\"CPU_do_stuff\"):\n",
    "    x_cpu *= 3\n",
    "    x_gpu *= x_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_cpu\n",
    "del x_gpu\n",
    "mempool.free_all_blocks()\n",
    "timer.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer('raw_kernel')\n",
    "z_dim = 4096\n",
    "yx_dims = (512, 512)\n",
    "# layer = np.arange(1,XY**2+1, dtype=np.float).reshape(XY,XY)\n",
    "with timer(\"CPU_create_array\"):\n",
    "    layer = np.random.rand(*yx_dims)\n",
    "    arr = np.stack([layer]*z_dim, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpu vs cpu](resources/cpu_vs_gpu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![struct](resources/gpu.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device 0: \"Quadro P6000\"\n",
    "\n",
    "| Param\t\t\t\t\t\t\t\t\t\t\t| Value\t\t\t\t\t\t\t\t|\n",
    "| --- | --- |\n",
    "| Total amount of global memory:                | 24450 MBytes (25637224448 bytes)\t\t\t\t\t\t\t\t|\n",
    "| (30) Multiprocessors, (128) CUDA Cores/MP:    | 3840 CUDA Cores\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| GPU Max Clock rate:                           | 1645 MHz (1.64 GHz)\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Warp size:                                    | 32\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Maximum number of threads per multiprocessor: | 2048\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Maximum number of threads per block:          | 1024\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max dimension size of a thread block (x,y,z): | (1024, 1024, 64)\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max dimension size of a grid size    (x,y,z): | (2147483647, 65535, 65535)\t\t\t\t\t\t\t\t\t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_size = 32\n",
    "\n",
    "g_size = tuple((d//warp_size+int(bool(d%warp_size)) for d in yx_dims[::-1]))\n",
    "b_size = (warp_size, warp_size)\n",
    "print(f'Grid size: {g_size}\\nBlock size: {b_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find two raw kernels:\n",
    "1. `mul_kernel` this kernel in the loop accumulate multiplication results straight into the output array. Output array is in global device memory, so access to this memory is slower than to the registry.\n",
    "2. `mul_kernel_reg` this kernel is accumulating the multiplication results into the registry variable `tmp`. For each step of the loop the access is faster for this variable than to the global memory. This kernel should be 10 time faster than mul_kernel.\n",
    "\n",
    "\n",
    "This example didn't show such time result during presentation because I changed the kernel code instead of creating two separate kernels (as it is right now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_kernel = cp.RawKernel(r'''\n",
    "    extern \"C\" __global__\n",
    "    void mul(const double* in, double* out, const int Z, const int Y, const int X) {\n",
    "        int x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "        if (x < X && y < Y) {\n",
    "            double tmp = 1.;\n",
    "            out[y*X+x] = 1.;\n",
    "            for (int z = 0; z < Z; z++)\n",
    "                out[y*X+x] = out[y*X+x] * in[z*Y*X+y*X+x];\n",
    "            //out[y*X+x] = tmp;\n",
    "        }\n",
    "    }\n",
    "    ''', 'mul')\n",
    "mul_kernel.compile()\n",
    "\n",
    "mul_kernel_reg = cp.RawKernel(r'''\n",
    "    extern \"C\" __global__\n",
    "    void mulReg(const double* in, double* out, const int Z, const int Y, const int X) {\n",
    "        int x = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int y = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "        if (x < X && y < Y) {\n",
    "            double tmp = 1.;\n",
    "            for (int z = 0; z < Z; z++)\n",
    "                tmp = tmp * in[z*Y*X+y*X+x];\n",
    "            out[y*X+x] = tmp;\n",
    "        }\n",
    "    }\n",
    "    ''', 'mulReg')\n",
    "mul_kernel_reg.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 0\n",
    "with timer(\"copy host-->dev\"):\n",
    "    in_arr = cp.asarray(arr, dtype=cp.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_arr_reg = cp.empty(yx_dims, dtype=cp.double)\n",
    "\n",
    "with timer(\"GPU__create_empty_output_array\"):\n",
    "    out_arr = cp.empty(yx_dims, dtype=cp.double)\n",
    "\n",
    "with timer(\"GPU_calc_mul\"):\n",
    "    mul_kernel(g_size, b_size, (in_arr, out_arr, *in_arr.shape))\n",
    "with Timer():\n",
    "    mul_kernel_reg(g_size, b_size, (in_arr, out_arr_reg, *in_arr.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"copy dev-->host\"):\n",
    "    out_gpu = cp.asnumpy(out_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"CPU_calc\"):\n",
    "    out_cpu = np.ones_like(layer)\n",
    "    for l in arr[:]:\n",
    "        out_cpu *= l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(out_gpu, out_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support for `__cuda_array_interface__`\\\n",
    "  The *cuda array interface* allows for interoperability between different implementation of GPU array-like objects in various projects\\\n",
    "  [https://numba.pydata.org/numba-doc/dev/cuda/cuda_array_interface.html](https://numba.pydata.org/numba-doc/dev/cuda/cuda_array_interface.html)\n",
    "- TensorFlow doesn't support it yet, but there is a hope\\\n",
    "  [https://github.com/tensorflow/tensorflow/issues/29039](https://github.com/tensorflow/tensorflow/issues/29039)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic",
   "language": "python",
   "name": "magic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
